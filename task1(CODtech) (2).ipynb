{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XluDbGhuO1u4",
        "outputId": "152c27b9-2102-4e83-f1ea-9c09116daf6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "# or you can use Pegasus: from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "\n",
        "def summarize_with_bart(text, max_input_length=1024, max_output_length=150):\n",
        "    model_name = \"facebook/bart-large-cnn\"  # For PEGASUS, use 'google/pegasus-xsum'\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    # Preprocess the text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "    # Generate the summary\n",
        "    summary_ids = model.generate(inputs['input_ids'], max_length=max_output_length, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== BART Summarizer ===\")\n",
        "    text_input = input(\"\\nPaste your article below:\\n\\n\")\n",
        "\n",
        "    print(\"\\n--- Generating Summary ---\\n\")\n",
        "    summary = summarize_with_bart(text_input)\n",
        "    print(\"SUMMARY:\\n\")\n",
        "    print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQSigxwRS85y",
        "outputId": "d235201e-9eb2-485d-ea94-5abd350ed296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== BART Summarizer ===\n",
            "\n",
            "Paste your article below:\n",
            "\n",
            "Qualities of artificial intelligence Although there is no uniformly agreed upon definition, AI generally is thought to refer to “machines that respond to stimulation consistent with traditional responses from humans, given the human capacity for contemplation, judgment and intention.”3 According to researchers Shubhendu and Vijay, these software systems “make decisions which normally require [a] human level of expertise” and help people anticipate problems or deal with issues as they come up.4 As such, they operate in an intentional, intelligent, and adaptive manner.  Intentionality Artificial intelligence algorithms are designed to make decisions, often using real-time data. They are unlike passive machines that are capable only of mechanical or predetermined responses. Using sensors, digital data, or remote inputs, they combine information from a variety of different sources, analyze the material instantly, and act on the insights derived from those data. With massive improvements in storage systems, processing speeds, and analytic techniques, they are capable of tremendous sophistication in analysis and decisionmaking.  Artificial intelligence is already altering the world and raising important questions for society, the economy, and governance.  Intelligence AI generally is undertaken in conjunction with machine learning and data analytics.5 Machine learning takes data and looks for underlying trends. If it spots something that is relevant for a practical problem, software designers can take that knowledge and use it to analyze specific issues. All that is required are data that are sufficiently robust that algorithms can discern useful patterns. Data can come in the form of digital information, satellite imagery, visual information, text, or unstructured data.  Adaptability AI systems have the ability to learn and adapt as they make decisions. In the transportation area, for example, semi-autonomous vehicles have tools that let drivers and vehicles know about upcoming congestion, potholes, highway construction, or other possible traffic impediments. Vehicles can take advantage of the experience of other vehicles on the road, without human involvement, and the entire corpus of their achieved “experience” is immediately and fully transferable to other similarly configured vehicles. Their advanced algorithms, sensors, and cameras incorporate experience in current operations, and use dashboards and visual displays to present information in real time so human drivers are able to make sense of ongoing traffic and vehicular conditions. And in the case of fully autonomous vehicles, advanced systems can completely control the car or truck, and make all the navigational decisions.\n",
            "\n",
            "--- Generating Summary ---\n",
            "\n",
            "SUMMARY:\n",
            "\n",
            "AI generally is thought to refer to “machines that respond to stimulation consistent with traditional responses from humans.” These software systems “make decisions which normally require [a] human level of expertise” Artificial intelligence algorithms are designed to make decisions, often using real-time data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the PEGASUS model and tokenizer\n",
        "model_name = \"google/pegasus-large\"  # Large model for improved accuracy\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def summarize_with_pegasus(text, max_input_length=1024, max_output_length=300):\n",
        "    # Function to split text into chunks based on token length\n",
        "    def split_text(text, max_length):\n",
        "        tokens = tokenizer.encode(text, truncation=False)\n",
        "        return [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
        "\n",
        "    # Split the input text into chunks\n",
        "    chunks = split_text(text, max_input_length)\n",
        "\n",
        "    # Summarize each chunk\n",
        "    summaries = []\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        # Prepare the input for the model\n",
        "        input_ids = torch.tensor([chunk]).to(model.device)\n",
        "\n",
        "        # Generate summary for the chunk\n",
        "        summary_ids = model.generate(input_ids,\n",
        "                                     max_length=max_output_length,\n",
        "                                     num_beams=4,\n",
        "                                     length_penalty=2.0,\n",
        "                                     early_stopping=True)\n",
        "\n",
        "        # Decode and add the summary to the list\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        summaries.append(summary)\n",
        "\n",
        "    # Combine chunk summaries into a final summary\n",
        "    final_summary = \" \".join(summaries)\n",
        "    return final_summary\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== PEGASUS Summarizer ===\")\n",
        "    text_input = input(\"\\nPaste your article below:\\n\\n\")\n",
        "\n",
        "    print(\"\\n--- Generating Summary ---\\n\")\n",
        "    summary = summarize_with_pegasus(text_input)\n",
        "    print(\"SUMMARY:\\n\")\n",
        "    print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msYr-IZrT4m9",
        "outputId": "ca4dd092-8604-4e40-b654-6b5e04663551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PEGASUS Summarizer ===\n",
            "\n",
            "Paste your article below:\n",
            "\n",
            "The number one function of Digit is to seize and manner tactile facts in real-time, allowing excessive-resolution touch notion. It includes an elastomer-based sensing surface that detects forces, vibrations, warmness, and even airborne chemicals. The embedded AI processor allows short analysis and response to tactile stimuli, making it an important component for robotic fingers and other interactive structures. Digit’s operation is based on a mixture of visual, thermal, and force sensing. Its excessive-decision sensor array captures minute details of floor textures and strain variations, making sure that even the smallest bodily interactions are detected. These signals are then processed the usage of an on-device AI neural network, which interprets touch styles and interprets them into significant records. This allows robots and different automatic structures to evolve to distinct environments with more precision. One of Digit’s key strengths is its potential to feature in actual-time. Unlike conventional tactile sensors that rely on external computing structures, Digit’s on-device processing minimizes latency, making it exceptionally responsive. This rapid feedback loop is in particular useful in dynamic environments inclusive of commercial automation and clinical robotics. The sensor’s capability to distinguish among soft and hard surfaces, hit upon item shapes, and degree pressure versions allows for stepped forward item handling and interaction. Another important aspect of the digit is its multimodal sensing capacity. Traditional touching sensors mainly focus on pressure detection, but extend beyond the digits of incorporating temperature sensitivity, chemical identity and vibration analysis. This creates an ideal solution for applications that require extensive environmental views. Whether it is detecting surface texture for quality control in the laboratory environment or identifying dangerous chemicals in the laboratory environment, the versatile functionality of the digit makes it a valuable tool in many domains. The mechanical design of the sensor includes a modular elastomermal, which improves stability by maintaining high sensitivity. Elastomas are designed to meet repeated interactions without significant decline, making the digit a long -term solution to require applications. In addition, the electronics of the digit have been specially designed to provide optimal camera properties, light control and video facility in a compact footprint. In addition, the points integrate deep learning models for improved functionality. By training the AI model with a wide data set, the sensor can learn to distinguish between different textures, predict success in robot applications and adjust the sensitivity based on environmental conditions. This level of adaptability makes it an important promoter for AI-operated robotics and automation.\n",
            "\n",
            "--- Generating Summary ---\n",
            "\n",
            "SUMMARY:\n",
            "\n",
            "The embedded AI processor allows short analysis and response to tactile stimuli, making it an important component for robotic fingers and other interactive structures. Elastomas are designed to meet repeated interactions without significant decline, making the digit a long -term solution to require applications. By training the AI model with a wide data set, the sensor can learn to distinguish between different textures, predict success in robot applications and adjust the sensitivity based on environmental conditions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quLkr2auUL4x",
        "outputId": "ad29bfb8-eb39-46cf-d91d-705cd6f37cde"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.30.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.1 (from gradio)\n",
            "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.30.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.30.0 gradio-client-1.10.1 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.10 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries for the summarizer and Gradio\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "# Load the PEGASUS model and tokenizer\n",
        "model_name = \"google/pegasus-large\"  # Large model for improved accuracy\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Function to split text into chunks based on token length\n",
        "def split_text(text, max_length):\n",
        "    # Ascertain the length of the input text\n",
        "    tokens = tokenizer.encode(text, truncation=False)\n",
        "    return [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
        "\n",
        "# Summarization function with customizable parameters\n",
        "def summarize_with_pegasus(text, max_input_length, max_output_length):\n",
        "    # Split the input text into chunks\n",
        "    chunks = split_text(text, max_input_length)\n",
        "\n",
        "    # Summarize each chunk\n",
        "    summaries = []\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        # Prepare the input for the model\n",
        "        input_ids = torch.tensor([chunk]).to(model.device)\n",
        "\n",
        "        # Generate summary for the chunk\n",
        "        summary_ids = model.generate(input_ids,\n",
        "                                   max_length=int(max_output_length),\n",
        "                                   num_beams=4,\n",
        "                                   length_penalty=2.0,\n",
        "                                   early_stopping=True)\n",
        "\n",
        "        # Decode and add the summary to the list\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        summaries.append(summary)\n",
        "\n",
        "    # Combine chunk summaries into a final summary\n",
        "    final_summary = \" \".join(summaries)\n",
        "    return final_summary\n",
        "\n",
        "# Function to count words in input text\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "# Sample article for testing\n",
        "def load_sample_article():\n",
        "    return \"\"\"\n",
        "    The rapid advancement of artificial intelligence (AI) has transformed industries worldwide, from healthcare to finance. AI systems, powered by machine learning and neural networks, can analyze vast datasets, make predictions, and automate tasks with unprecedented efficiency. However, concerns about ethical implications, job displacement, and data privacy remain significant. Governments and organizations are now investing heavily in AI research to balance innovation with regulation, ensuring AI benefits society while minimizing risks.\n",
        "    \"\"\"\n",
        "\n",
        "# Gradio interface function\n",
        "def summarize_text(article_text, max_input_length, max_output_length, progress=gr.Progress()):\n",
        "    # Check if input is empty\n",
        "    if not article_text.strip():\n",
        "        return None, \"Please enter some text to summarize.\", count_words(\"\")\n",
        "\n",
        "    # Validate input parameters\n",
        "    if max_input_length < 100 or max_output_length < 50:\n",
        "        return None, \"Please set max input length >= 100 and max output length >= 50.\", count_words(article_text)\n",
        "\n",
        "    # Generate and return the summary\n",
        "    try:\n",
        "        progress(0.1, desc=\"Summarizing your article...\")\n",
        "        summary = summarize_with_pegasus(article_text, max_input_length, max_output_length)\n",
        "        return summary, None, count_words(article_text)\n",
        "    except Exception as e:\n",
        "        return None, f\"An error occurred: {str(e)}\", count_words(article_text)\n",
        "\n",
        "# Custom CSS for a polished, aesthetic look\n",
        "custom_css = \"\"\"\n",
        "body {\n",
        "    font-family: 'Arial', sans-serif;\n",
        "}\n",
        ".gr-button {\n",
        "    border-radius: 12px !important;\n",
        "    padding: 12px 24px !important;\n",
        "    font-weight: 600 !important;\n",
        "    transition: all 0.3s ease !important;\n",
        "}\n",
        ".gr-button:hover {\n",
        "    opacity: 0.9 !important;\n",
        "    transform: scale(1.02) !important;\n",
        "}\n",
        ".gr-textbox {\n",
        "    border-radius: 12px !important;\n",
        "    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1) !important;\n",
        "    padding: 15px !important;\n",
        "}\n",
        ".gr-slider {\n",
        "    background: linear-gradient(90deg, #e0e7ff, #c7d2fe) !important;\n",
        "    border-radius: 8px !important;\n",
        "}\n",
        ".card {\n",
        "    background: #ffffff !important;\n",
        "    border-radius: 16px !important;\n",
        "    box-shadow: 0 6px 12px rgba(0, 0, 0, 0.15) !important;\n",
        "    padding: 20px !important;\n",
        "    margin-bottom: 20px !important;\n",
        "}\n",
        ".header {\n",
        "    background: linear-gradient(135deg, #6366f1, #a855f7) !important;\n",
        "    color: white !important;\n",
        "    padding: 20px !important;\n",
        "    border-radius: 12px !important;\n",
        "    text-align: center !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Define the Gradio interface with a modern, aesthetic design\n",
        "with gr.Blocks(theme=gr.themes.Monochrome(), css=custom_css) as interface:\n",
        "    # Header\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <div class=\"header\">\n",
        "            <h1>PEGASUS Text Summarizer</h1>\n",
        "            <p>Create concise summaries of articles using advanced AI. Paste your text, tweak settings, and get a polished summary instantly.</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        # Input Section\n",
        "        with gr.Column(scale=3):\n",
        "            with gr.Group(elem_classes=\"card\"):\n",
        "                gr.Markdown(\"### 📝 Enter Your Article\")\n",
        "                article_input = gr.Textbox(\n",
        "                    label=\"Article Text\",\n",
        "                    placeholder=\"Paste or type your article here...\",\n",
        "                    lines=12,\n",
        "                    max_lines=20,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "                word_count = gr.Textbox(\n",
        "                    label=\"Word Count\",\n",
        "                    value=\"0\",\n",
        "                    interactive=False,\n",
        "                    elem_classes=\"card\"\n",
        "                )\n",
        "                with gr.Row():\n",
        "                    sample_button = gr.Button(\"Load Sample Article\", variant=\"secondary\")\n",
        "                    clear_button = gr.Button(\"Clear\", variant=\"secondary\")\n",
        "\n",
        "        # Settings and Output Section\n",
        "        with gr.Column(scale=2):\n",
        "            with gr.Group(elem_classes=\"card\"):\n",
        "                gr.Markdown(\"### ⚙️ Summarization Settings\")\n",
        "                max_input_length = gr.Slider(\n",
        "                    minimum=100,\n",
        "                    maximum=2048,\n",
        "                    value=1024,\n",
        "                    step=100,\n",
        "                    label=\"Max Input Length (tokens)\",\n",
        "                    info=\"Size of text chunks processed.\"\n",
        "                )\n",
        "                max_output_length = gr.Slider(\n",
        "                    minimum=50,\n",
        "                    maximum=500,\n",
        "                    value=300,\n",
        "                    step=10,\n",
        "                    label=\"Max Output Length (tokens)\",\n",
        "                    info=\"Length of each summary chunk.\"\n",
        "                )\n",
        "                summarize_button = gr.Button(\"Summarize\", variant=\"primary\")\n",
        "\n",
        "            with gr.Group(elem_classes=\"card\"):\n",
        "                gr.Markdown(\"### 📄 Summary\")\n",
        "                summary_output = gr.Textbox(\n",
        "                    label=\"Generated Summary\",\n",
        "                    placeholder=\"Your summary will appear here...\",\n",
        "                    lines=8,\n",
        "                    max_lines=12,\n",
        "                    interactive=False\n",
        "                )\n",
        "                error_output = gr.Textbox(\n",
        "                    label=\"Status\",\n",
        "                    placeholder=\"Status messages will appear here...\",\n",
        "                    interactive=False,\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "    # Event handlers\n",
        "    summarize_button.click(\n",
        "        fn=summarize_text,\n",
        "        inputs=[article_input, max_input_length, max_output_length],\n",
        "        outputs=[summary_output, error_output, word_count]\n",
        "    )\n",
        "    sample_button.click(\n",
        "        fn=load_sample_article,\n",
        "        inputs=None,\n",
        "        outputs=[article_input]\n",
        "    )\n",
        "    article_input.change(\n",
        "        fn=count_words,\n",
        "        inputs=article_input,\n",
        "        outputs=word_count\n",
        "    )\n",
        "    clear_button.click(\n",
        "        fn=lambda: (\"\", \"0\", None, None),\n",
        "        inputs=None,\n",
        "        outputs=[article_input, word_count, summary_output, error_output]\n",
        "    )\n",
        "\n",
        "# Launch the Gradio interface\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "qSmutycLTafu",
        "outputId": "e8849c42-55da-42ac-e7da-8381c8f56cd8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://72f102e53fc50080a5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://72f102e53fc50080a5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"google/pegasus-large\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Function to chunk text with slight overlap\n",
        "def chunk_text(text, max_tokens=1024, overlap=50):\n",
        "    tokens = tokenizer.encode(text, truncation=False)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), max_tokens - overlap):\n",
        "        chunk = tokens[i:i + max_tokens]\n",
        "        chunks.append(chunk)\n",
        "        if i + max_tokens >= len(tokens):\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "# Summarization function\n",
        "def summarize(text, max_input_length=1024, max_output_length=150):\n",
        "    chunks = chunk_text(text, max_input_length)\n",
        "    partial_summaries = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        input_ids = torch.tensor([chunk])\n",
        "        summary_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_output_length,\n",
        "            num_beams=4,\n",
        "            length_penalty=2.0,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        partial_summaries.append(summary)\n",
        "\n",
        "    # Stage 2: Combine partial summaries and summarize again\n",
        "    combined_summary = \" \".join(partial_summaries)\n",
        "    final_input_ids = tokenizer.encode(combined_summary, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    final_summary_ids = model.generate(final_input_ids, max_length=150, num_beams=4, length_penalty=2.0)\n",
        "    final_summary = tokenizer.decode(final_summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "# CLI interface\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== OPTIMIZED SUMMARIZER ===\")\n",
        "    input_text = input(\"\\nPaste your full article:\\n\\n\")\n",
        "    print(\"\\n--- Summarizing... Please wait ---\\n\")\n",
        "    summary = summarize(input_text)\n",
        "    print(\"\\nFINAL SUMMARY:\\n\")\n",
        "    print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edwSp1VOa9WV",
        "outputId": "453cbca4-794e-4a2d-f191-b0f30618fcff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== OPTIMIZED SUMMARIZER ===\n",
            "\n",
            "Paste your full article:\n",
            "\n",
            "The number one function of Digit is to seize and manner tactile facts in real-time, allowing excessive-resolution touch notion. It includes an elastomer-based sensing surface that detects forces, vibrations, warmness, and even airborne chemicals. The embedded AI processor allows short analysis and response to tactile stimuli, making it an important component for robotic fingers and other interactive structures. Digit’s operation is based on a mixture of visual, thermal, and force sensing. Its excessive-decision sensor array captures minute details of floor textures and strain variations, making sure that even the smallest bodily interactions are detected. These signals are then processed the usage of an on-device AI neural network, which interprets touch styles and interprets them into significant records. This allows robots and different automatic structures to evolve to distinct environments with more precision. One of Digit’s key strengths is its potential to feature in actual-time. Unlike conventional tactile sensors that rely on external computing structures, Digit’s on-device processing minimizes latency, making it exceptionally responsive. This rapid feedback loop is in particular useful in dynamic environments inclusive of commercial automation and clinical robotics. The sensor’s capability to distinguish among soft and hard surfaces, hit upon item shapes, and degree pressure versions allows for stepped forward item handling and interaction. Another important aspect of the digit is its multimodal sensing capacity. Traditional touching sensors mainly focus on pressure detection, but extend beyond the digits of incorporating temperature sensitivity, chemical identity and vibration analysis. This creates an ideal solution for applications that require extensive environmental views. Whether it is detecting surface texture for quality control in the laboratory environment or identifying dangerous chemicals in the laboratory environment, the versatile functionality of the digit makes it a valuable tool in many domains. The mechanical design of the sensor includes a modular elastomermal, which improves stability by maintaining high sensitivity. Elastomas are designed to meet repeated interactions without significant decline, making the digit a long -term solution to require applications. In addition, the electronics of the digit have been specially designed to provide optimal camera properties, light control and video facility in a compact footprint. In addition, the points integrate deep learning models for improved functionality. By training the AI model with a wide data set, the sensor can learn to distinguish between different textures, predict success in robot applications and adjust the sensitivity based on environmental conditions. This level of adaptability makes it an important promoter for AI-operated robotics and automation.\n",
            "\n",
            "--- Summarizing... Please wait ---\n",
            "\n",
            "\n",
            "FINAL SUMMARY:\n",
            "\n",
            "By training the AI model with a wide data set, the sensor can learn to distinguish between different textures, predict success in robot applications and adjust the sensitivity based on environmental conditions.\n"
          ]
        }
      ]
    }
  ]
}